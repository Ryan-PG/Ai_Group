{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### تنظیمات اولیه\n",
    "در این بخش، پارامترهای اولیه الگوریتم Q-Learning تعریف می‌شوند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# تنظیمات اولیه\n",
    "gamma = 0.9  # ضریب تخفیف\n",
    "alpha = 0.1  # نرخ یادگیری\n",
    "epsilon = 0.1  # احتمال انجام عمل تصادفی (جستجو در برابر بهره‌برداری)\n",
    "\n",
    "# تعریف محیط (شبکه 5x5)\n",
    "n_states = 25  # تعداد کل حالات (5x5)\n",
    "n_actions = 4  # تعداد کل اعمال (بالا، پایین، چپ، راست)\n",
    "goal_state = 24  # هدف در آخرین خانه\n",
    "actions = ['up', 'down', 'left', 'right']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### تعریف Q-Table\n",
    "در این قسمت، جدول Q برای نگهداری مقادیر Q برای هر حالت و عمل تعریف می‌شود.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# تعریف Q-Table\n",
    "Q = np.zeros((n_states, n_actions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### تابعی برای محاسبه حالت بعدی و پاداش\n",
    "این تابع، با توجه به وضعیت فعلی و عملی که انتخاب می‌شود، وضعیت بعدی و پاداش آن را محاسبه می‌کند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# تابعی برای محاسبه حالت بعدی و پاداش\n",
    "def step(state, action):\n",
    "    row, col = divmod(state, 5)  # تبدیل ایندکس به مختصات\n",
    "    if action == 0 and row > 0:  # بالا\n",
    "        row -= 1\n",
    "    elif action == 1 and row < 4:  # پایین\n",
    "        row += 1\n",
    "    elif action == 2 and col > 0:  # چپ\n",
    "        col -= 1\n",
    "    elif action == 3 and col < 4:  # راست\n",
    "        col += 1\n",
    "    next_state = row * 5 + col  # تبدیل دوباره مختصات به ایندکس\n",
    "    reward = 1 if next_state == goal_state else -0.01  # پاداش 1 اگر هدف رسید، در غیر این صورت -0.01\n",
    "    return next_state, reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### اجرای الگوریتم Q-Learning\n",
    "در این بخش، الگوریتم Q-Learning برای آموزش عامل بر روی محیط تعریف شده اجرا می‌شود.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# اجرای الگوریتم Q-Learning\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    state = random.randint(0, n_states - 1)  # شروع از یک حالت تصادفی\n",
    "    while state != goal_state:\n",
    "        # انتخاب عمل (جستجو در برابر بهره‌برداری)\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.randint(0, n_actions - 1)  # عمل تصادفی\n",
    "        else:\n",
    "            action = np.argmax(Q[state])  # بهترین عمل\n",
    "        \n",
    "        # اجرای عمل و دریافت پاداش\n",
    "        next_state, reward = step(state, action)\n",
    "        \n",
    "        # به‌روزرسانی Q-Table\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "        \n",
    "        state = next_state  # انتقال به حالت بعدی\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### نمایش Q-Table\n",
    "در اینجا جدول Q که عامل طی آموزش آن را یاد گرفته است، نمایش داده می‌شود.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table:\n",
      "[[ 0.02486045 -0.01009294 -0.01045297  0.41037123]\n",
      " [ 0.01279772  0.03786872  0.03502297  0.4841769 ]\n",
      " [ 0.16215587  0.15990678  0.05612543  0.54953454]\n",
      " [ 0.19547802  0.62170994  0.2200676   0.23222073]\n",
      " [-0.004901    0.68440638  0.04978397  0.07694901]\n",
      " [-0.00837281  0.03972902 -0.00916606  0.45870967]\n",
      " [ 0.05457105  0.54887115  0.07504132  0.04201428]\n",
      " [-0.00468317  0.03396159  0.04468689  0.61645986]\n",
      " [ 0.18749086  0.7019      0.17377106  0.12394631]\n",
      " [ 0.08583514  0.79092534  0.00461579  0.23336492]\n",
      " [-0.00398179  0.03930775  0.03954338  0.53910999]\n",
      " [ 0.14862749  0.16314599  0.23660536  0.62170929]\n",
      " [ 0.07169874  0.44099864  0.18631425  0.7019    ]\n",
      " [ 0.45340241  0.791       0.39352154  0.65751183]\n",
      " [ 0.38378806  0.88999998  0.28659943  0.30594048]\n",
      " [ 0.03801806  0.07388986  0.01774936  0.58554322]\n",
      " [ 0.04863219 -0.00387333  0.02774258  0.70125936]\n",
      " [ 0.11025344  0.23378948  0.10068081  0.79099903]\n",
      " [ 0.55854215  0.7022164   0.44937792  0.89      ]\n",
      " [ 0.72325311  1.          0.73969417  0.73906052]\n",
      " [ 0.08349234  0.00293669  0.06235503  0.66261955]\n",
      " [ 0.02181695  0.23869014  0.001845    0.7905168 ]\n",
      " [ 0.13285647  0.20570683  0.22589005  0.88999135]\n",
      " [ 0.09814034  0.0780581   0.35247992  0.99999998]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# نمایش Q-Table\n",
    "print(\"Q-Table:\")\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### آزمایش عامل آموزش‌دیده\n",
    "در این قسمت، عامل آموزش‌دیده به شروع از خانه اول می‌پردازد و مسیر خود را تا رسیدن به هدف طی می‌کند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "مسیر به سمت هدف: [0, 1, 2, 3, 8, 13, 18, 19, 24]\n"
     ]
    }
   ],
   "source": [
    "# آزمایش عامل آموزش‌دیده\n",
    "state = 0  # شروع از اولین خانه\n",
    "path = [state]\n",
    "while state != goal_state:\n",
    "    action = np.argmax(Q[state])  # انتخاب بهترین عمل\n",
    "    state, _ = step(state, action)\n",
    "    path.append(state)\n",
    "\n",
    "print(\"\\nمسیر به سمت هدف:\", path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
