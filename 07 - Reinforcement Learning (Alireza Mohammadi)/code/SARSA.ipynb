{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### تنظیمات اولیه\n",
    "در این بخش، پارامترهای اولیه الگوریتم Q-Learning تعریف می‌شوند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# تنظیمات اولیه\n",
    "gamma = 0.9  # ضریب تخفیف\n",
    "alpha = 0.1  # نرخ یادگیری\n",
    "epsilon = 0.1  # احتمال انتخاب عمل تصادفی (Exploration vs Exploitation)\n",
    "\n",
    "# تعریف محیط (جدول 5x5)\n",
    "n_states = 25  # تعداد کل حالات (5x5)\n",
    "n_actions = 4  # تعداد کل اعمال (بالا، پایین، چپ، راست)\n",
    "goal_state = 24  # هدف در آخرین خانه\n",
    "actions = ['up', 'down', 'left', 'right']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### تعریف Q-Table\n",
    "در این قسمت، جدول Q برای نگهداری مقادیر Q برای هر حالت و عمل تعریف می‌شود.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# تعریف Q-Table\n",
    "Q = np.zeros((n_states, n_actions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### تابعی برای محاسبه حالت بعدی و پاداش\n",
    "این تابع، با توجه به وضعیت فعلی و عملی که انتخاب می‌شود، وضعیت بعدی و پاداش آن را محاسبه می‌کند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# تابعی برای محاسبه حالت بعدی و پاداش\n",
    "def step(state, action):\n",
    "    row, col = divmod(state, 5)  # تبدیل ایندکس به ردیف و ستون\n",
    "    if action == 0 and row > 0:  # حرکت به بالا\n",
    "        row -= 1\n",
    "    elif action == 1 and row < 4:  # حرکت به پایین\n",
    "        row += 1\n",
    "    elif action == 2 and col > 0:  # حرکت به چپ\n",
    "        col -= 1\n",
    "    elif action == 3 and col < 4:  # حرکت به راست\n",
    "        col += 1\n",
    "    next_state = row * 5 + col  # تبدیل دوباره ردیف و ستون به ایندکس\n",
    "    reward = 1 if next_state == goal_state else -0.01  # پاداش 1 اگر هدف رسید، در غیر این صورت -0.01\n",
    "    return next_state, reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### اجرای الگوریتم SARSA\n",
    "در این بخش، الگوریتم SARSA برای آموزش عامل بر روی محیط تعریف شده اجرا می‌شود.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# الگوریتم SARSA\n",
    "episodes = 1000  # تعداد اپیزودها برای آموزش\n",
    "for episode in range(episodes):\n",
    "    state = random.randint(0, n_states - 1)  # شروع از یک حالت تصادفی\n",
    "    \n",
    "    # انتخاب عمل اولیه\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        action = random.randint(0, n_actions - 1)  # انتخاب عمل تصادفی (جستجو)\n",
    "    else:\n",
    "        action = np.argmax(Q[state])  # انتخاب بهترین عمل (استفاده از اطلاعات قبلی)\n",
    "\n",
    "    while state != goal_state:\n",
    "        # اجرای عمل و دریافت پاداش\n",
    "        next_state, reward = step(state, action)\n",
    "        \n",
    "        # انتخاب عمل بعدی (a') برای حالت بعدی (s')\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            next_action = random.randint(0, n_actions - 1)  # انتخاب عمل تصادفی (جستجو)\n",
    "        else:\n",
    "            next_action = np.argmax(Q[next_state])  # انتخاب بهترین عمل (استفاده از اطلاعات قبلی)\n",
    "        \n",
    "        # به‌روزرسانی Q-table با استفاده از الگوریتم SARSA\n",
    "        Q[state, action] = Q[state, action] + alpha * (\n",
    "            reward + gamma * Q[next_state, next_action] - Q[state, action]\n",
    "        )\n",
    "        \n",
    "        # انتقال به حالت و عمل بعدی\n",
    "        state = next_state\n",
    "        action = next_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### نمایش Q-Table\n",
    "در اینجا جدول Q که عامل طی آموزش آن را یاد گرفته است، نمایش داده می‌شود.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table:\n",
      "[[-0.0126868  -0.00936192 -0.01347368  0.20390928]\n",
      " [ 0.06788093  0.29082534 -0.01083267 -0.01026732]\n",
      " [-0.00741958  0.4037036   0.02812211  0.06008175]\n",
      " [ 0.03308344 -0.00563537 -0.00593044  0.47750081]\n",
      " [ 0.09466373  0.60457671  0.08156098  0.12108797]\n",
      " [-0.01214427  0.25283546 -0.01187582  0.02166681]\n",
      " [ 0.0628309   0.01989408  0.01303053  0.40365199]\n",
      " [ 0.10234776  0.52952136  0.10213352  0.12831645]\n",
      " [-0.0014911   0.13532236  0.17589967  0.62673772]\n",
      " [ 0.15094995  0.75837134  0.14075186  0.35028727]\n",
      " [ 0.0120356   0.06525298  0.02634345  0.4481053 ]\n",
      " [ 0.05331236  0.4810969   0.0817965   0.06626379]\n",
      " [ 0.18544333  0.64459586  0.05200946  0.27248506]\n",
      " [ 0.09527286  0.70441382  0.16677818  0.07242323]\n",
      " [ 0.15577662  0.85256079  0.11627755  0.20160926]\n",
      " [ 0.030588    0.0217068  -0.0065549   0.48593445]\n",
      " [ 0.18558352  0.57406294  0.07894695  0.07895927]\n",
      " [ 0.26816244  0.42162406  0.28083853  0.73885924]\n",
      " [ 0.23391532  0.50603093  0.38022571  0.82456505]\n",
      " [ 0.55805566  1.          0.60574213  0.6302987 ]\n",
      " [ 0.05206921 -0.00559311  0.0380703   0.52222126]\n",
      " [ 0.18251023  0.33951455  0.15238565  0.70848878]\n",
      " [ 0.28162666  0.46421764  0.30778421  0.85228741]\n",
      " [ 0.45584047  0.17787234  0.4547554   1.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# نمایش Q-Table\n",
    "print(\"Q-Table:\")\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### آزمایش عامل آموزش‌دیده\n",
    "در این قسمت، عامل آموزش‌دیده به شروع از خانه اول می‌پردازد و مسیر خود را تا رسیدن به هدف طی می‌کند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "مسیر به سمت هدف: [0, 1, 6, 7, 12, 17, 18, 19, 24]\n"
     ]
    }
   ],
   "source": [
    "# آزمایش عامل آموزش‌دیده\n",
    "state = 0  # شروع از اولین حالت (گوشه بالا-چپ)\n",
    "path = [state]\n",
    "while state != goal_state:\n",
    "    action = np.argmax(Q[state])  # انتخاب بهترین عمل\n",
    "    state, _ = step(state, action)  # انتقال به حالت بعدی\n",
    "    path.append(state)\n",
    "\n",
    "print(\"\\nمسیر به سمت هدف:\", path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
